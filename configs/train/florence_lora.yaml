output_dir: ./checkpoints/florence2-chartqa-lora
learning_rate: 1e-4
batch_size: 4
gradient_accumulation_steps: 4
num_train_epochs: 5
warmup_ratio: 0.03
weight_decay: 0.01
max_grad_norm: 1.0
logging_steps: 10
eval_steps: 100
save_steps: 100
save_total_limit: 2
dataloader_num_workers: 4
bf16: false
fp16: true
optim: adamw_torch
report_to: wandb
lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  bias: none
  target_modules: 
    - q_proj
    - v_proj
    - k_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
max_new_tokens: 32
generation_num_beams: 3